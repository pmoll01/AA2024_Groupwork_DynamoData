{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define time series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Store evaluation metrics for each fold\n",
    "cv_results = {'rmse': [], 'mae': []}\n",
    "\n",
    "# Prepare the data for cross-validation\n",
    "time_series = data['utilizationRate_total']\n",
    "\n",
    "# Perform time series cross-validation\n",
    "for train_index, test_index in tscv.split(time_series):\n",
    "    train, test = time_series.iloc[train_index], time_series.iloc[test_index]\n",
    "\n",
    "    # Fit ARIMA model for each fold\n",
    "    model_cv = ARIMA(train, order=(2, 1, 2))\n",
    "    fitted_model_cv = model_cv.fit()\n",
    "    \n",
    "    # Predict on the test set\n",
    "    forecast_cv = fitted_model_cv.forecast(steps=len(test))\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    rmse = np.sqrt(mean_squared_error(test, forecast_cv))\n",
    "    mae = mean_absolute_error(test, forecast_cv)\n",
    "    \n",
    "    # Store results\n",
    "    cv_results['rmse'].append(rmse)\n",
    "    cv_results['mae'].append(mae)\n",
    "\n",
    "# Calculate average metrics across folds\n",
    "avg_rmse = np.mean(cv_results['rmse'])\n",
    "avg_mae = np.mean(cv_results['mae'])\n",
    "\n",
    "# Train the final model on the full training set\n",
    "final_train_data = time_series[:-720]  # Exclude last 720 hours as holdout set\n",
    "final_test_data = time_series[-720:]  # Holdout set\n",
    "final_model = ARIMA(final_train_data, order=(2, 1, 2))\n",
    "final_fitted_model = final_model.fit()\n",
    "\n",
    "# Predict on the holdout set\n",
    "final_forecast = final_fitted_model.forecast(steps=len(final_test_data))\n",
    "\n",
    "# Calculate performance metrics on holdout set\n",
    "final_rmse = np.sqrt(mean_squared_error(final_test_data, final_forecast))\n",
    "final_mae = mean_absolute_error(final_test_data, final_forecast)\n",
    "\n",
    "# Combine actual and predicted values for visualization\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': final_test_data,\n",
    "    'Predicted': final_forecast\n",
    "}, index=final_test_data.index)\n",
    "\n",
    "# Plot actual vs. predicted values\n",
    "plt.figure(figsize=(12, 6))\n",
    "results_df['Actual'].plot(label='Actual (Holdout Set)', color='blue')\n",
    "results_df['Predicted'].plot(label='Predicted (ARIMA)', color='red')\n",
    "plt.title(\"ARIMA Model - Actual vs. Predicted (Holdout Set)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Utilization Rate (%)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Display cross-validation results and holdout set performance\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Average RMSE (CV)', 'Average MAE (CV)', 'Holdout RMSE', 'Holdout MAE'],\n",
    "    'Value': [avg_rmse, avg_mae, final_rmse, final_mae]\n",
    "})\n",
    "tools.display_dataframe_to_user(name=\"Cross-Validation and Holdout Set Performance Metrics\", dataframe=metrics_df)\n",
    "\n",
    "# Display actual vs predicted values\n",
    "tools.display_dataframe_to_user(name=\"Actual vs Predicted Utilization (Holdout Set)\", dataframe=results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DynamoData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
